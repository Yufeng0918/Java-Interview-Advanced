## 生产实践



## 1. 服务框架, 注册中心, 网关系统

部署几台机器，每台机器的配置是什么，每天整体的流量有多少，高峰期的请求量有多少，你的整体系统是否抗住了? 各个服务之间的超时、重试、幂等，是否搞定了

死扣生产细节问题，每秒3000并发请求，各个服务部署多少机器，机器的配置，数据库的机器和配置，网关的机器和配置，注册中心的机器和配置，什么样的机器配置能抗多大的流量和请求



### 服务中心

服务上线，注册表多级缓存同步1秒，注册表拉取频率降低为1秒
服务心跳，1秒上报1次
故障发现，1秒钟检查一次1心跳，如果发现2秒内服务没上报心跳，认为故障了

+ 服务注册中心都没任何压力，最多就是每秒钟几十次请求而已
+ 服务注册中部署个2台机器，每台机器就是4核8G，高可用冗余，任何一台机器死掉，不会影响系统的运行
+ 服务注册中心这样的一个处理逻辑，4核8G的机器，每秒钟轻松抗几百请求，上千请求也是可以的

通常来说，如果每秒钟的并发在1000以内的话，很少部署的，每个服务部署2台机器，每台机器4核8G，每台机器每秒抗个几百请求。大部分的系统，高峰期每秒几百请求，低峰期每秒几十请求，甚至几个请求



### 网关服务

网关系统，4核8G的机器，一台机器抗每秒几百请求，部署3~4台机器，保证可以网关系统每台机器的压力比较小，进一步保证网关系统可靠性



### 数据库

数据库，32核64G，最多抗个每秒钟几千请求问题不大，平时抗个每秒钟几十或者几百请求，三四千请求，但是只不过此时会导致MySQL机器的负载很高，CPU使用率很高，磁盘IO负载很高，网络负载很高





## 2. 并发流量

每个服务每天多少请求量，高峰期每秒钟多少请求量，任何一个开源系统都需要对自己运行过程中各种请求量、每秒的请求量、成功次数、失败次数，在内存里直接做一些计数，他会给你开放一些端口号，比如http端口号，你只要请求他这个端口号，他就会把这些metrics统计返回给你



在你负责的核心服务里，核心接口，开发一个简单的metric统计机制，AtomicLong，原子性，并发下数据统计准确，不会错误，每个接口被调用的时候，一个是可以对每个接口每分钟都做一个Metric统计。 对每个接口每天的请求使用一个AtomicLong做一个计数，统计出来每天的请求次数。 计算一下每个接口从请求到执行完毕，需要耗费多长时间，算一下每个接口平均的请求延时，TP99，TP95，TP90，TP50，TP99，99%的请求耗费的时间在100ms以内，但是1%的请求可能耗费的时间在100ms以上

**TP99 = 100ms**
**TP95 = 50ms，95%的请求耗费的时间多在50ms以内，但是5%的请求耗费的时间在50ms以上**

平均响应延时, 可以计算出来这个接口平均响应延时，把每次调用的耗时跟历史总耗时加起来，除以当前的请求次数，不就是最新的接口响应平均延时

**高峰**: 260K, 70mins, 10pods, averg 0.5s





## 3. 服务扩容

访问量扩大10倍，如何扩容

**网关扩容**直接多部署10倍的机器即可，前面的Nginx做会负载均衡，把流量均匀分发给各个网关机器

**注册中心扩容**服务实例变多了10倍，此时几十个服务实例，几百个服务实例，**对eureka机器会造成每秒几百请求**，没问题，eureka机器，8核16G的配置，单机抗上千请求，很轻松

**数据库扩容**，数据库本来是每秒几百请求，10倍，每秒高峰期是三四千请求，横向扩容很麻烦，可以考虑给单个数据库部署的机器提高配置，32核128G高配物理机，每秒钟抗几千请求问题不大

